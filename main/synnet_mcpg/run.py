"""
Generates synthetic trees where the root molecule optimizes for a specific objective
based on Therapeutic Data Commons (TDC) oracle functions. Uses a MCPG style deterministic
REINFORCE algorithm with genetic algorithm as local search to optimize embeddings before decoding.
"""
import inspect
import math
import os
import random

import numpy as np
from matplotlib import pyplot as plt
from tqdm import tqdm
import sys

path_here = os.path.dirname(os.path.realpath(__file__))
sys.path.append(path_here)
sys.path.append('.')
from main.optimizer import BaseOptimizer, Objdict

from syn_net.utils.ga_utils import crossover, mutation
import multiprocessing as mp
import numpy as np
import pandas as pd
import time
import scripts._mp_decode as decode
from syn_net.utils.predict_utils import mol_fp
from tqdm import tqdm
import torch as th
import torch.nn as nn
from torch.nn.utils import clip_grad_norm_
from main.synnet_mcpg.syn_net.utils.graph_utils import update_xs_by_vs, pick_xs_by_vs


TEN = th.Tensor
gpu_id = 0
device = th.device(f'cuda:{gpu_id}' if th.cuda.is_available() and gpu_id >= 0 else 'cpu')


def fitness_np(embs, _pool, obj):
    results = _pool.map(decode.func, embs)
    # print("fitness decode finished")
    ### debug mode: without pool (parallel)
    # results = []
    # for emb in embs:
    #     results.append(decode.func(emb))
    ### debug mode

    smiles = [r[0] for r in results]
    trees = [r[1] for r in results]
    scores = [obj(smi) if smi is not None else 0.0 for smi in smiles]
    return scores, smiles, trees


def fitness(embs, _pool, obj):
    """
    Returns the scores for the root molecules in synthetic trees generated by the
    input molecular embeddings.

    Args:
        embs (list): Contains molecular embeddings (vectors).
        _pool (mp.Pool): A pool object, which represents a pool of workers (used
            for multiprocessing).
        obj (str): The objective function to use to compute the fitness.

    Raises:
        ValueError: Raised if the specified objective function is not implemented.

    Returns:
        scores (list): Contains the scores for the root molecules in the
            generated trees.
        smiles (list): Contains the root molecules encoded as SMILES strings.
        trees (list): Contains the synthetic trees generated from the input
            embeddings.
    """
    embs = embs.detach().cpu().numpy()
    results = _pool.map(decode.func, embs)
    # print("fitness decode finished")
    ### debug mode: without pool (parallel)
    # results = []
    # for emb in embs:
    #     results.append(decode.func(emb))
    ### debug mode

    smiles = [r[0] for r in results]
    trees = [r[1] for r in results]
    scores = [obj(smi) if smi is not None else 0.0 for smi in smiles]
    return TEN(scores).to(device), np.array(smiles), trees


def distribution_schedule(n, total):
    """
    Determines the type of probability to use in the `crossover` function, based
    on the number of generations which have occured.

    Args:
        n (int): Number of elapsed generations.
        total (int): Total number of expected generations.

    Returns:
        str: Describes a type of probability distribution.
    """
    if n < 4 * total / 5:
        return 'linear'
    else:
        return 'softmax_linear'


def num_mut_per_ele_scheduler(n, total):
    """
    Determines the number of bits to mutate in each vector, based on the number
    of elapsed generations.

    Args:
        n (int): Number of elapsed generations.
        total (int): Total number of expected generations.

    Returns:
        int: Number of bits to mutate.
    """
    return 24


def mut_probability_scheduler(n, total):
    """
    Determines the probability of mutating a vector, based on the number of elapsed
    generations.

    Args:
        n (int): Number of elapsed generations.
        total (int): Total number of expected generations.

    Returns:
        float: The probability of mutation.
    """
    if n < total / 2:
        return 0.5
    else:
        return 0.5


class PolicyORG(nn.Module):
    def __init__(self, num_bits: int):
        super().__init__()
        self.num_nodes = num_bits
        self.out = nn.Parameter(th.rand((1, self.num_nodes)) * 0.02 + 0.49, requires_grad=True)

    def forward(self):
        return th.sigmoid(self.out)


class SynNet_MCPG_Optimizer(BaseOptimizer):
    def metropolis_hastings_sampling(self, probs: TEN, start_xs: TEN, num_repeats: int, num_iters: int = 24) -> TEN:
        # metropolis-hastings sampling: Monte Carlo sampling using transition kernel in Markov Chain with accept ratio
        xs = start_xs.repeat(num_repeats, 1)  # Parallel
        ps = probs.repeat(num_repeats, 1)

        num, dim = xs.shape
        device = xs.device

        count = 0
        for j in range(
                4):  # After 4 rounds of iteration, even if many nodes are rejected, no further iterations will be performed.
            ids = th.randperm(dim,
                              device=device)  # Sample nodes in a random order, selecting them uniformly. Avoid sampling different nodes different numbers of times.
            for i in ids:
                idx = ids[i]
                chosen_p0 = ps[0, idx]
                chosen_xs = xs[:, idx]
                chosen_ps = th.where(chosen_xs, chosen_p0, 1 - chosen_p0)

                accept_rates = (1 - chosen_ps) / chosen_ps
                accept_masks = th.rand(num, device=device).lt(accept_rates)
                xs[:, idx] = th.where(accept_masks, th.logical_not(chosen_xs), chosen_xs)

                count += accept_masks.sum()
                if count >= num * num_iters:
                    break
            if count >= num * num_iters:
                break
        return xs

    def local_search_inplace(self, good_xs: TEN,
                             num_iters: int = 2):
        with mp.Pool(processes=self.config.ncpu) as pool:
            vs_raw, ms_raw, _ = fitness(good_xs, pool, self.oracle)

        dist_ = "softmax_linear"
        num_mut_per_ele_ = 24
        mut_probability_ = 0.5
        num_offspring = len(good_xs) * 6

        population = good_xs.detach().cpu().numpy()
        scores = vs_raw.detach().cpu().numpy().tolist()

        for ga_i in range(num_iters):
            population_old = population.copy()
            scores_old = scores.copy()
            offspring = crossover(parents=population,
                                  offspring_size=num_offspring,
                                  distribution=dist_)
            offspring = mutation(offspring_crossover=offspring,
                                 num_mut_per_ele=num_mut_per_ele_,
                                 mut_probability=mut_probability_)

            new_population = np.unique(np.concatenate([population, offspring], axis=0), axis=0)

            with mp.Pool(processes=self.config.ncpu) as pool:
                new_scores, new_mols, trees = fitness_np(new_population, pool, self.oracle)
            new_scores = np.array(new_scores)
            scores = []
            mols = []

            parent_idx = 0
            indices_to_print = []
            while parent_idx < len(population):
                max_score_idx = np.where(new_scores == np.max(new_scores))[0][0]
                if new_mols[max_score_idx] not in mols:
                    indices_to_print.append(max_score_idx)
                    scores.append(new_scores[max_score_idx])
                    mols.append(new_mols[max_score_idx])
                    population[parent_idx, :] = new_population[max_score_idx, :]
                    new_scores[max_score_idx] = -999999
                    parent_idx += 1
                else:
                    if new_scores[max_score_idx] == -999999:
                        print(
                            f"GA error {ga_i}/{num_iters}, {parent_idx} < {len(population)}, total {self.ga_error_count} errors")
                        population = population_old
                        scores = scores_old
                        self.ga_error_count += 1
                        break
                    new_scores[max_score_idx] = -999999

        good_xs = TEN(population).to(device) == 1
        good_vs = TEN(scores).to(device)
        # print(f"local search with {num_iters}, improvement: {th.mean(good_vs - vs_og)}")
        return good_xs, good_vs, ms_raw

    def __init__(self, args=None):
        super().__init__(args)
        self.model_name = "synnet"

    def _optimize(self, oracle, config):
        print("**MCPG w/ GA local search**, budget:", self.oracle.budget)

        self.ga_error_count = 0
        self.oracle.assign_evaluator(oracle)
        config = Objdict(config)
        self.config = config

        if config['restart']:
            population = np.load(config.input_file)
            print(f"Starting with {len(population)} fps from {config.input_file}")
        else:
            try:
                starting_smiles = pd.read_csv(config.input_file).sample(config.num_population)
                starting_smiles = starting_smiles['smiles'].tolist()
                population = np.array(
                    [mol_fp(smi, config['radius'], config['nbits']) for smi in starting_smiles]
                )
                # print(population.shape, 'population shape ------')
                # population = population.reshape((population.shape[0], population.shape[2]))
                print(f"Starting with {len(starting_smiles)} fps from {config.input_file}")
            except:
                starting_smiles = self.all_smiles[:config.num_population]
                population = np.array(
                    [mol_fp(smi, config.radius, config.nbits) for smi in starting_smiles]
                )
                # population = np.ceil(np.random.random(size=(config['num_population'], config['nbits'])) * 2 - 1)
                print(f"Starting with {config.num_population} fps with {config.nbits} bits")

        population = TEN(population).to(device)

        # Evaluation initial population
        best_xs = population == 1

        with mp.Pool(processes=config.ncpu) as pool:
            best_vs, best_mols, trees = fitness(best_xs, _pool=pool, obj=oracle)

        score_x = th.argsort(best_vs, descending=True)
        best_xs = best_xs[score_x]
        best_mols = best_mols[score_x.detach().cpu()]
        best_vs = best_vs[score_x]

        print(f"Initial: {best_vs.mean():.3f} +/- {best_vs.std():.3f}")
        # print(f"Scores: {best_vs}")

        learning_rate = .1
        weight_decay = 1e-4
        num_iters = self.config.num_iters
        reset_gap = num_iters
        entropy_weight = 1
        num_searchers = 1
        num_repeats = self.config.num_repeats
        if_maximize = True

        num_nodes = 4096
        assert num_nodes == 4096

        '''model'''
        net = PolicyORG(num_bits=num_nodes).to(device)
        net_params = list(net.parameters())

        optimizer = th.optim.Adam(net_params, lr=learning_rate, maximize=False) if weight_decay \
            else th.optim.AdamW(net_params, lr=learning_rate, maximize=False, weight_decay=weight_decay)

        '''loop'''
        th.set_grad_enabled(False)
        lambda_entropy = (th.cos(
            th.arange(reset_gap, device=device) / (reset_gap - 1) * th.pi) + 1) / 2 * entropy_weight

        for i in range(num_iters):
            if self.ga_error_count >= 10:
                print(f'convergence met at gen {i}, abort ...... ')
                print(config)
                break

            t = time.time()
            # evolutionary_replacement(best_xs, best_vs, low_k=2, if_maximize=if_maximize)

            if self.finish:
                print(f'MCPG: max oracle hit at gen {i}, abort ...... ')
                print(config)
                break

            '''prob'''
            probs = net()  # shape == (num_sims, num_nodes)

            '''full'''
            full_xs = self.metropolis_hastings_sampling(probs=probs, start_xs=best_xs, num_repeats=num_repeats,
                                                        num_iters=self.config.sampling_iters)
            full_vs = None
            full_mols = None

            for _ in range(num_searchers):
                full_xs, full_vs, full_mols = self.local_search_inplace(good_xs=full_xs, num_iters=self.config.ga_iters)

            advantages = full_vs.float()
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

            good_xs, good_vs, good_ms = pick_xs_by_vs(xs=full_xs, vs=full_vs, ms=full_mols, num_repeats=num_repeats,
                                                      if_maximize=if_maximize)
            # print(good_xs.shape)
            update_xs_by_vs(xs0=best_xs, vs0=best_vs, ms0=best_mols, xs1=good_xs, vs1=good_vs, ms1=good_ms,
                            if_maximize=if_maximize)
            del full_vs

            '''gradient'''
            th.set_grad_enabled(True)  # ↓↓↓↓↓↓↓↓↓↓ gradient

            probs = net()  # shape == (num_sims, num_nodes)

            full_ps = probs.repeat(len(population) * num_repeats, 1)

            logprobs = th.log(th.where(full_xs, full_ps, 1 - full_ps)).sum(dim=1)
            print(logprobs.shape)
            _probs = 1 - probs
            entropy = (probs * probs.log2() + _probs * _probs.log2()).mean(dim=1)  # entropy, - Useful, more stable.
            obj_entropy = entropy.mean()
            obj_values = (th.softmax(logprobs, dim=0) * advantages).sum()

            objective = obj_values + obj_entropy * lambda_entropy[i % reset_gap]
            optimizer.zero_grad()
            objective.backward()
            clip_grad_norm_(net_params, 3)
            optimizer.step()
            th.set_grad_enabled(False)  # ↑↑↑↑↑↑↑↑↑ gradient
            # print(f"entropy {obj_entropy:9.4f}")

            score_x = th.argsort(best_vs, descending=True)
            best_xs = best_xs[score_x]
            best_mols = best_mols[score_x.detach().cpu()]
            best_vs = best_vs[score_x]
            print(f"Generation {i + 1}: {best_vs.mean():.3f} +/- {best_vs.std():.3f}",
                  f"Consumed time: {(time.time() - t):.3f} s")
